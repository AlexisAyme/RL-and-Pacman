{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réalisation d'un algorithme de Reinforcement Learning (RL)\n",
    "\n",
    "## Table des matières :\n",
    "\n",
    "## Introduction \n",
    "\n",
    "## I : Problèmes étudiés\n",
    "### a : Chaines de Markov et récompenses\n",
    "### b : Politique \n",
    "### c : Implémentation et tests\n",
    "\n",
    "\n",
    "## II : Programmation dynamique : une première approche\n",
    "### a : Equation de Bellman, fonction valeur \n",
    "### b : Algorithme de détermination de la fonction valeur\n",
    "### c : Application au plus court chemin\n",
    "\n",
    "## III : Deux algorithmes d'apprentissage : le TD-Lambda et le Q-Learning\n",
    "### a : Le TD-Lambda\n",
    "### b : Le Q-Learning\n",
    "### c : Implémentation\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### a : Conclusion générale\n",
    "### b : Apports personnels\n",
    "#### i : Apports personnels Alexis Ayme\n",
    "#### ii : Apports personnels Philippe Cantrelle\n",
    "#### iii : Apports personnels Hélène Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Le Machine Learning, s’appuie sur des raisonnements mathématiques, traduits informatiquement dans des algorithmes capables de digérer de grandes quantités d’informations pour acquérir de nouvelles connaissances ou encore comprendre un comportement. Egalement appelé apprentissage automatisé, le principe du Machine Learning consiste à pouvoir apprendre en toute autonomie à partir de données et d’évoluer en permanence. Ces fonctions d’apprentissage automatique détectent des schémas clé et y ajustent leur fonctionnement. Concrètement, à chaque interaction avec l’application, un clic ou  un « like » par exemple, l’application est capable d’en déduire un comportement et d’ajuster en fonction le contenu proposé et son interface. Cette mécanique suscite aujourd’hui beaucoup d’engouement chez le grand public. Le ML en tant que tel est très complexe à mettre en place. \n",
    "    \n",
    "Quatre grands types d'apprentissage existent au sein du Machine Learning, à savoir :\n",
    "    -L'apprentissage supervisé\n",
    "    -L'apprentissage semi-supervisé\n",
    "    -L'apprentissage non supervisé\n",
    "    -L'apprentissage par renforcement\n",
    "    \n",
    "Nous avons décidé d'être, pour un premier projet de programmation, ambitieux et avons pris le risque de nous attaquer à un sujet de Machine Learning. Nous nous sommes en effet consacré à l'apprentissage par renforcement, également appelé Reinforcement Learning en anglais (RL). L’apprentissage par renforcement correspond au cas où l'algorithme apprend un comportement étant donnée une observation. L'action de l'algorithme sur l'environnement produit une valeur de retour qui guide l'algorithme d'apprentissage. \n",
    "    \n",
    "Lorsqu’il y a un problème, la machine est censée décider de la meilleure action à effectuer en fonction de son état actuel.  Lorsque cette étape est répétée, le problème est connu comme étant un processus de décision de Markov. De façon optimale, l’apprentissage par renforcement utilise des processus de décision de Markov, mais l'idée de base est tout simplement de saisir les aspects les plus importants du vrai problème face à une machine en interaction avec son environnement, pour atteindre un objectif. De toute évidence, un tel agent doit être capable de détecter l'état de l'environnement dans une certaine mesure et doit être capable de prendre des mesures qui affectent l'état. \n",
    "    \n",
    "Nous nous sommes appuyé sur le cours de Rémi Munos, lui-même disponible sur le site de Xavier Dupré, pour réaliser notre projet. Nous avons tout d'abord compris les notions de bases du Reinforcement Learning et nous nous sommes intéressés à différents problèmes. Nous avons ensuite effectuer une première approche, en utilisant la programmation dynamique. Enfin, nous nous sommes intéressés à un algorithme d'apprentissage : le TD-lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I : Problèmes étudiés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a : Chaines de Markov et récompenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d’abord, définissons le type de problème que nous allons chercher à résoudre. \n",
    "\n",
    "\n",
    "Nous étudierons des situations dans lesquelles l’ordinateur évoluera au sein d’un environnement. Dans cet environnement, on supposera que l’ordinateur se trouve dans un état initial. Il a alors la possibilité d’effectuer plusieurs actions. Après avoir choisi l’une d’entre elles, il se retrouvera dans un nouvel état (qui peut éventuellement être le même que le précédent) avec une certaine probabilité, et récoltera la récompense correspondante. Ainsi, l’ordinateur accumule les récompenses au fil de son parcours, jusqu’à atteindre un état terminal au-delà duquel plus aucune action n’est possible.\n",
    "Pour plus de commodité, introduisons quelques notations :\n",
    "\n",
    "\n",
    "$X$ l’ensemble des états.\n",
    "\n",
    "\n",
    "$A(s)$ l’ensemble des actions possibles à partir de l’état s et $A$ l’ensemble regroupant toutes les actions possibles.\n",
    "\n",
    "\n",
    "En notant $x_1,…,x_n$ et $a_1,…,a_n$ les variables aléatoires correspondant respectivement aux $n$ premiers états et les $n$ premières actions, on supposera vérifiée la propriété des chaînes de Markov :\n",
    "\n",
    "\n",
    "$$∀i∈\\{1,…,n-1\\},   P(x_{i+1}│x_{1},…,x_{i},a_{1},…,a_{i})=P(x_{i+1}│x_{i},a_{i} )$$\n",
    "\n",
    "\n",
    "Autrement dit, la variable $x_{i+1}$ ne dépend que de $x_{i}$ et de $a_{i}$, les états et les actions de l’étape précédente. Ainsi, on peut définir la probabilité de transition de $y$ à $x$ sous l’action $a$ comme étant :\n",
    "\n",
    "\n",
    "$$T(y,a,x)=P(x_{i+1}=x|x_{i}=y,a_{i}=a)$$\n",
    "\n",
    "\n",
    "Dans la suite et dans les algorithmes, on considèrera la matrice T de transition qui vérifie :\n",
    "$$T:X×A×X→[0,1]$$  et  $$T[x_i,a,x_{i+1} ]=T(x_{i},a,x_{i+1} )$$\n",
    "\n",
    "\n",
    "On notera également $r(x,a)$ la variable aléatoire correspondant à la récompense que l’on obtient en effectuant dans l’état $x$ l’action $a$. Dans la suite et dans les algorithmes, les récompenses seront données sous la forme d’une matrice R.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b : Politique \n",
    "\n",
    "On appelle politique (déterministe) $\\Pi $ une application de $S$ vers $A^*$.\n",
    "De façon succinte, il s'agit du choix d'action que va faire notre individu.\n",
    "\n",
    "Exemple : notre individu, qui suit la politique $\\Pi$, est dans l'état $s$. Il va donc effectuer l'action $\\Pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c : Implémentation et tests\n",
    "\n",
    "### Premier test unitaire \n",
    "\n",
    "On va tester notre algorithme VI sur un cas simple où la politique optimale est facile à justifier.\n",
    "\n",
    "\n",
    "On considère un segment de $n$ cases. L'individu se trouvant sur une case $i$ a deux choix :\n",
    "soit il se tourne vers la gauche, soit vers à droite.\n",
    "\n",
    "\n",
    "Si il est à droite il a une proba d'avancer de $0,75$ et de reculer de $0,25$ et inversement si il est tourné vers la gauche.\n",
    "\n",
    "\n",
    "A chaque fois qu'il bouge d'une case il reçoit une récompense $-i$\n",
    "\n",
    "\n",
    "La case $0$ est terminale.\n",
    "\n",
    "\n",
    "La bonne stratégie pour maximiser la récompense est donc de toujours se tourner à gauche ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II : Programmation dynamique : une première approche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a : Equation de Bellman, fonction valeur\n",
    "\n",
    "Nous allons résoudre dans un premier temps le problème en admettant que les probabilités de transition sont connues. Nous allons résoudre le problème en utilisant une équation célèbre de la programmation dynamique : l'équation de Bellman. \n",
    "\n",
    "\n",
    "On peut définir la fonction valeur $V^{\\pi}$ comme la somme actualisée d'une constante $\\gamma$ (entre 0 et 1) des récompenses futures anticipées d'un individu partant de x: \n",
    "\n",
    "$$ V^{\\pi} (x)= \\mathbb{E}[\\sum_{t=0}^{\\infty} \\gamma^t r(x_t,\\pi(x_t))|x_0=x, \\pi]$$\n",
    "\n",
    "Notons $V^*$ la valeur maximale (de la meilleure politique): $$ V^* = \\text{max}_{\\pi} V^{\\pi} $$\n",
    "\n",
    "Pour la meilleure politique, choisissons la notation $\\pi^*$.\n",
    "\n",
    "On dispose alors de l'équation de Bellman (qui se démontre grâce à la propriété des chaînes de Markov) :\n",
    "$$ V^{\\pi} (s)= \\mathbb{E}( r(s,\\pi(s))) +\\gamma\\sum_{s'\\in X} P(s \\stackrel{\\pi(x)}{\\longrightarrow} s') V^{\\pi} (s')$$\n",
    "\n",
    "Ainsi  :\n",
    "\n",
    "$$V^*(s)= \\max_{a\\in A(s)}[\\mathbb{E}( r(s,a)) +\\gamma\\sum_{s'\\in X} P(s \\stackrel{a}{\\longrightarrow} s') V^* (s')]$$\n",
    "\n",
    "On reconnait alors une équation du type $V^*=f(V^*)$\n",
    "Il ne reste alors plus qu'à utiliser une méthode de point fixe (par exemple celle de Picard marche très bien si $\\gamma <1$) pour trouver $V^*$.\n",
    "\n",
    "Pour simplifier d'un poil le prochain algorithme, on suppose que $r(x,\\pi(x))$ est une constante (ie : la récompense n'est plus une var). L'algorithme marche aussi si $r(x,\\pi(x))$ n'est pas une constante, il faudra juste l'appliquer en spécifiant $r(x,\\pi(x))$ à son espérance avant d'exécuter l'algorithme.\n",
    "\n",
    "\n",
    "Les équations de Bellman donnent alors :\n",
    "\n",
    "$$V^*(s)= \\max_{a\\in A(s)}[ r(s,a) +\\gamma\\sum_{s'\\in X} P(s \\stackrel{a}{\\longrightarrow} s') V^* (s')]$$\n",
    "\n",
    "Nous allons donc coder en deux étapes et en dimension finie ( la fonction $V^*$ devient un vecteur de $\\mathbb{R}^n$) :\n",
    "\n",
    "\n",
    "Etape 1 : coder $f$ \n",
    "\n",
    "\n",
    "Etape 2 : itérer $f$ jusqu'à avoir convergence \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norme (X):\n",
    "    \"\"\"Donne le carré de la norme euclidienne du vecteur X\"\"\"\n",
    "    return sum (X*X)\n",
    "\n",
    "class Value_iterative :\n",
    "    def __init__ (self,T,A,R,gamma):\n",
    "        self.m_T = np.copy (T)\n",
    "        self.m_A = np.copy (A)\n",
    "        self.m_R = np.copy (R)\n",
    "        self.m_gamma = gamma\n",
    "        \n",
    "    def f (self, V):\n",
    "        \"\"\"La fonction donnée par l'équation de Bellman sur la value fonction retourne f(V) et la politique choisie\"\"\"\n",
    "        n=len(V)\n",
    "        resV =np.zeros(n)\n",
    "        pi=np.zeros(n)\n",
    "        for i in range (n):\n",
    "            m= 0\n",
    "            b= False \n",
    "            for a in self.m_A[i]:\n",
    "                S= self.m_R[i,a] + self.m_gamma*sum ([(self.m_T[i,a,j]*V[j]) for j in range(n)])\n",
    "                if m< S or b==False :\n",
    "                    m=S \n",
    "                    b=True\n",
    "                    pi[i]=a\n",
    "            resV[i]= m\n",
    "        return (resV,pi)\n",
    "    \n",
    "    def V_and_pi_star (self,epsilon): \n",
    "        \"\"\"Itération de f pour converger vers le point fixe\"\"\"\n",
    "        n=len(self.m_A)\n",
    "        V = np.zeros(n)\n",
    "        (Vp,pi_res) = self.f(V)\n",
    "        while norme (V-Vp)> epsilon :\n",
    "            W=Vp\n",
    "            (Vp,pi_res)=self.f(Vp)\n",
    "            V=W\n",
    "        return (Vp,pi_res)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 remarques : \n",
    "\n",
    "Cet algorithme converge très rapidement si $\\gamma <1$ (théorème de Picard)\n",
    "\n",
    "\n",
    "Cet algorithme fonctionne également si $\\gamma = 1$ et que toute trajectoire converge presque sûrement vers un état final, par contre la vitesse de convergence est moins évidente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b : Algorithme de détermination de la fonction valeur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c : Application au plus court chemin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III : Deux algorithmes d'apprentissage : le TD-Lambda et le Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a : Le TD-Lambda\n",
    "\n",
    "\n",
    "Une première solution serait de tester un grand nombre de fois cette boîte noire et d'utiliser le résultat des tests dans le calcul :\n",
    "\n",
    "\n",
    "Rappel: Méthode de Monte-Carlo, soit $(Y_i)_{i\\in \\mathbb{N}}$ i.i.d, et loi des grand nombres\n",
    "$$ \\frac{Y_1+Y_2+...+Y_n}{n} \\stackrel{p.s}{\\longrightarrow} \\mathbb{E}(Y_1) $$\n",
    "\n",
    "\n",
    "en spécifiant\n",
    "$$Y_i = \\sum_{t=0}^{\\infty} \\gamma^t r(x_t^{(i)},\\pi(x_t^{(i)}))$$\n",
    "où $(x_t^{(i)})$ sont des trajectoires aléatoires où $ x_0^{(i)}= x$.\n",
    "\n",
    "\n",
    "On a alors, \n",
    "\n",
    "$$\\frac{Y_1+Y_2+...+Y_n}{n} \\stackrel{p.s}{\\longrightarrow} V^{\\pi}(x)$$\n",
    "\n",
    "Que l'on peut écrire de manière séquentielle :\n",
    "\n",
    "$$ V_{k+1}(x)= (1-\\eta_{k}) V_{k}(x) + \\eta_{k}\\sum_{t=0}^{\\infty} \\gamma^t r(x_t^{(k)},\\pi(x_t^{(k)}))$$\n",
    "\n",
    "où $\\eta_{k} = \\frac{1}{k}$ ou plus généralement terme générale d'une série $L^1$ divergente et $L^2$ convergente.\n",
    "\n",
    "\n",
    "Cette suite $((V_{k}(x))$ converge alors vers $V^{\\pi}(x)$.\n",
    "\n",
    "\n",
    "Néanmoins si on code directement la formule ci-dessus le coût en terme de complexité risque d'être assez élevé. En effet, on doit simuler pour chaque $x$  un grand nombre de trajectoires. Cependant en bougeant un petit peu l'écriture on remarque que l'on peut réutiliser des trajectoires pour d'autre états : \n",
    "\n",
    "$$V_{k+1}(x_s)= (1-\\eta_{k}) V_{k}(x_s) + \\eta_{k}\\sum_{t=s}^{\\infty} \\gamma^{t-s} r(x_{t},\\pi(x_{t}))$$\n",
    "\n",
    "on note, $r_t =r(x_{t},\\pi(x_{t}))$,\n",
    "\n",
    "\n",
    "$$V_{k+1}(x_s)= (1-\\eta_{k}) V_{k}(x_s) + \\eta_{k}\\sum_{t=s}^{\\infty} \\gamma^{t-s} r_t$$\n",
    "\n",
    "qui devient,\n",
    "\n",
    "$$V_{k+1}(x_s)=  V_{k}(x_s) + \\eta_{k}\\sum_{t=s}^{\\infty} \\gamma^{t-s} (r_t + \\gamma V_k(x_{t+1})- V_k(x_{t}))$$\n",
    "\n",
    "La quantité $\\delta_t(V_k) = r_t + \\gamma V_k(x_{t+1})- V_k(x_{t})$ est la différence temporelle. On remarque qu'elle ne dépend que de ce qu'il y a en $k$ et d'une nouvelle récompense $r_t$. On peut aussi vérifier que $\\mathbb{E}(\\delta_t(V_k))=0 $ grâce à l'équation de Bellman.\n",
    "\n",
    "\n",
    "Donc on a, \n",
    "$$V_{k+1}(x_s)=  V_{k}(x_s) + \\eta_{k}\\sum_{t=s}^{\\infty} \\gamma^{t-s} \\delta_t(V_k)$$\n",
    "\n",
    "Nous sommes donc heureux, nous avons une équation que nous allons pouvoir exploiter de la même manière que ce que nous avons fait avec l'opérateur de Bellman.\n",
    "\n",
    "Néanmoins, ici, notre opérateur est aléatoire et la convergence est presque sûre. \n",
    "\n",
    "Cependant, nous avons un gros avantage ! \n",
    "\n",
    "Nous avons en effet un algorithme qui va pouvoir directement intéragir avec l'environnemnt mais sans en connaitre le fonctionnement à l'avance.\n",
    "\n",
    "\n",
    "On va coder l'algorithme dit du TD lambda, qui calcule les itération de : \n",
    "$$V_{k+1}(x_s)=  V_{k}(x_s) + \\eta_{k}\\sum_{t=s}^{\\infty} (\\gamma \\lambda)^{t-s} \\delta_t(V_k)$$ \n",
    "\n",
    "Voici un rapide résumé de l'algorithme suivant, qui marche quand les trajectoires se terminent quasi-sûrement et dans un temps raisonnable : \n",
    "\n",
    "On dispose d'un tableau des occurences pour parcourir uniformement tous les états.\n",
    "\n",
    "1) on prend un état initial, on visite à partir de cet état \n",
    "\n",
    "\n",
    "2) si un état est visité on rajoute 1 dans le tableau des occurences \n",
    "\n",
    "\n",
    "3) on arrête l'algorithme si chaque état est visité un bon nombre de fois.\n",
    "\n",
    "D'abord on réalise une petite fonction qui choisit une valeur dans $1,...n$ suivant le tableau des occurences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choisir (occ):\n",
    "    n= len(occ)\n",
    "    s= sum(occ)\n",
    "    tab = np.array([s for i in range(n)])- np.array(occ)\n",
    "    som=tab[0]\n",
    "    u= np.random.random() * s \n",
    "    i=0\n",
    "    while som<u :\n",
    "        i+=1\n",
    "        som+=tab[i]\n",
    "        \n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction va nous permettre de bien choisir le début de nos chemins. On peut passer à la fonction principale, valable dans le cas suivant : \n",
    "\n",
    "1) $\\gamma = 1$\n",
    "\n",
    "\n",
    "2) Les trajectoires se terminent en un temps raisonnable\n",
    "\n",
    "\n",
    "3) $\\eta_{k} = \\frac{1}{k}$\n",
    "\n",
    "\n",
    "Dans cet algorithme, on considère les trajectoires à la première visite (pas de biais) car on prendra des petites dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons également la trace d'éligibilité $z_n \\in \\mathbb{R^\\mathbb{N}} $\n",
    "\n",
    "Une fois la transition $z_n$ à $z_{n+1}$ observée, on peut calculer la différence temporelle $d_k = r^{\\pi}(x_k) + V_n(x_{k+1}) -V_n(x_k)$.\n",
    "\n",
    "On met ensuite la trace d'éligibilité à jour :\n",
    "\n",
    "$z_{n}(x) = \\left \\{\\begin{array}{r c l} \n",
    "\\lambda z_{n-1} (x)  & \\mbox{si x différent de x_k }\\\\ \n",
    "1+\\lambda z_{n-1}(x) & \\mbox{si x = x_k}\\\\ \n",
    "0 & \\mbox{si x_k=0 (remise à zéro des traces)}\\\\ \n",
    "\\end{array} \\right $\n",
    "\n",
    "Puis on itère (pour tout x) :  $V_{n+1}(x) = V_{n}(x) + \\eta_{n}(x) z_{n}(x)d_n $\n",
    "\n",
    "Quid du choix de $\\lambda$ ?\n",
    "\n",
    "Prendre un $\\lambda$ strictement inférieur à 1 permet de réduire la variance des estimateurs, le prendre strictement positif permet de propager plus rapidement les récompenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b : Le Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons une séquence de $Q$-valeurs $Q_n$  (fonctions définies sur l'espace produit $X x A$) de la manière suivante : lors d'une transition d'un état $x$ vers $y$ $\\sim$ $p(.|x,a)$ en ayant choisi l'action a, et observé la récompense r, on itère la fonction Q-valeur de l'état (x,a) selon : \n",
    "\n",
    "$Q_{n+1}(x,a) = (1 - \\eta_{n} (x,a))Q_{n}(x,a) + \\eta_{n}(x,a)[r + \\underset{b \\in A}{max} Q_n(y,b)].$\n",
    "\n",
    "Supposons que toutes les politiques sont propres et que tous les états-actions (x,a) sont itérés une une infinité de fois et que les pas satisfont, $\\forall (x,a)$, $\\underset{n \\geqslant 0}{\\sum} \\eta_{n} (x,a) = \\infty, \\underset{n \\geqslant 0}{\\sum} \\eta_{n}^{2} (x,a) < \\infty. $ Alors $\\forall x \\in X, a \\in A, Q_{n}(x,a) \\overset{p.s}{\\rightarrow} Q^{*}(x,a).$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c : Implémentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## a : Conclusion générale\n",
    "\n",
    "Le principal objectif du projet était de se familiariser avec la méthode du Q-learning et avec le Machine Learning dans sa globalité. Le projet semblait très ambitieux, peut-être même trop pour Hélène et Philippe qui avaient peur de se sentir dépassé par le sujet. Néanmoins, Alexis fut une véritable locomotive pour le groupe et son expérience a été plus que profitable à tous les membres du groupe. Il n'a en effet jamais hésité à aider ses deux acolytes, à leur expliquer les notions mathématiques, à répondre aux nombreuses interrogations, etc.\n",
    "    \n",
    "Nous étions initialement hésitant sur notre sujet. L'algorithme des fourmis nous paraissait peut-être plus accessible. Random Forest nous semblait tout aussi intéressant que le sujet que nous finalement choisi. Notre choix nous a finalement conduit vers un sujet plus ambitieux et plus compliqué, sans doute guidé par l'envie d'en apprendre plus sur le Machine Learning dans sa globalité, ainsi que par la confiance prodiguée par l'expérience déjà acquise par Alexis. \n",
    "    \n",
    "Ce projet nous a également appris à travailler sur un sujet de programmation en groupe, à partager des fichiers, à documenter le code pour se faire comprendre des autres, à échanger, à partager. \n",
    "    \n",
    "Même si les heures passées dessus furent nombreuses, c'est avec joie que nous nous tournons désormais vers ce travail accompli.\n",
    "\n",
    "## b : Apports personnels\n",
    "\n",
    "### i : Apports personnels Alexis Ayme\n",
    "\n",
    "Ayant déjà programmé auparavant, ce projet m'a permis de découvrir de façon concrète un sujet qui me faisait de l'oeil, d'un point de vue personnel, depuis quelques mois déjà. Je suis très heureux d'avoir appronfondi le RL et de me l'être mieux approprié. J'ai aussi appris à diriger une petite équipe composée de novices, à la fois en les guidant et en leur fournissant des pistes. Alors que j'étais plutôt habitué à travailler seul sur mes projets informatiques, cela m'a beaucoup apporté sur ce domaine.\n",
    "\n",
    "### ii : Apports personnels Philippe Cantrelle\n",
    "\n",
    "Novice en programmation, mon expérience se limitait à l'IPT (informatique pour tous) de CPGE. Partir sur le RL me faisait un peu peur mais, rétrospectivement, je suis très heureux d'avoir, avec Hélène et Alexis, pris ce sujet. Cela m'a premièrement permis de démystifer le Machine Learning, me permettant de me rendre compte que, même si le sujet est compliqué, il n'est pas impossible d'en comprendre les mécanismes. J'ai  également appris à utiliser Jupyter et le notebook, à mettre en pratique le concept de classes et à mieux cerner les tests unitaires. \n",
    "\n",
    "\n",
    "### iii : Apports personnels Hélène Wang\n",
    "\n",
    "Bien qu'issue de classes préparatoires MP, je n'avais pas réellement d'expérience en programmation avant de choisir le projet d'informatique, et c'est d'ailleurs pour cela que je l'ai fait. Grâce à ce projet, j'ai pu comprendre les principes sur lesquels le RL se base et mettre en pratique ce que nous avons étudié au premier semestre, notamment les classes. Le projet a de plus fait appel à des notions mathématiques que je n'avais pas rencontrées jusque-là (comme par exemple l'équation de Bellman) et que j'ai donc pu découvrir. C'est aussi la première fois que j'ai utilisé les notebooks ainsi que Github et j'ai pu constaté que ce sont des outils très pratiques pour travailler à plusieurs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
